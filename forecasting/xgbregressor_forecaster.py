import pylab as pl
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import sklearn
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor
import copy
import os
import warnings
import seaborn as sns
from matplotlib.pylab import rcParams
rcParams['figure.figsize'] = 12, 6

class XGBRegressorForecaster:
    '''
    A class to run an XGBRegressor boosted regression tree on the pre-processed data frame
    generated by feature_engineering.py.
    '''
    
    def __init__(self, dataframe, target_column='mean_transaction_amount', train_columns=None):
        self.n_estimators = 100
        self.max_depth = 20
        self.dataframe = dataframe
        self.target_column = target_column
        if train_columns != None:
            self.train_columns = train_columns
        else:
            self.train_columns = [col for col in self.dataframe.columns if 
                                  '_customer_number_startswith_A' in col]
            self.train_columns += [col for col in self.dataframe.columns if '_day_of_year' in col]
            self.train_columns += [col for col in self.dataframe.columns if '_day_of_month' in col]
            self.train_columns += [col for col in self.dataframe.columns if '_season' in col]
            self.train_columns += [col for col in self.dataframe.columns if '_program_code' in col]
            self.train_columns += [col for col in self.dataframe.columns if '_program_year' in col]
            self.train_columns += [col for col in self.dataframe.columns if '_state_code' in col]
            self.train_columns += [col for col in self.dataframe.columns if '_county_code' in col]
            self.train_columns += [col for col in self.dataframe.columns if '_days_of_drought' in col]
            self.train_columns += [col for col in self.dataframe.columns if '_drought_severity' in col]
            if 'mean_futures_30day_std' in self.dataframe.columns:
                self.train_columns += [col for col in self.dataframe.columns if '_futures' in col]
    
    def form_training_and_test_sets(self, training_fraction=0.66):
        '''
        A function to form training and test sets from the input data frame.
        By default the fraction of data in the training set (training_fraction)
        is 0.66, with the final 0.34 of the time-ordered data resderved for 
        testing via the walk_forward_forecast() function defined below.
        '''
        self.X = self.dataframe[self.train_columns].values
        self.y = self.dataframe[self.target_column].values
        # reserve final third of the time-ordered data for testing
        self.split_idx = int(round(len(self.y) * training_fraction))
        self.X_train = self.X[:self.split_idx, :]
        self.X_test = self.X[self.split_idx:, :]
        self.y_train = self.y[:self.split_idx]
        self.y_test = self.y[self.split_idx:]
    
    # Hyperparameter grid search functions:
    
    def evaluate_xgb_model(self, max_depth, n_estimators):
        '''
        A helper function for the do_hyperparameter_grid_search() function. 
        Given specified n_estimator and max_depth hyperparameters, it tests 
        an XGBRegressor model tuned to the speficied hyperparameters.
        Output: the root mean squared error for the model.
        '''
        predictions = []
        X, y = self.X_train, self.y_train
        split_idx = int(round(0.66 * X.shape[0]))
        X_test = X[split_idx:, :]
        y_test = y[split_idx:]
        for i in range(len(y_test)):
            model = XGBRegressor(max_depth=max_depth, n_estimators=n_estimators)
            model.fit(X[:split_idx+i,:], y[:split_idx+i])
            output = model.predict(X_test[i,:].reshape((1,-1)))
            prediction = output[0]
            predictions.append(prediction)
        mse = mean_squared_error(y_test, predictions)
        return np.sqrt(mse)
    
    def do_hyperparameter_grid_search(self, verbose=True):
        '''
        A function to perform a hyperparameter grid search to identify the
        optimal n_estimators and max_depth hyperparameters for the XGBRegressor 
        model.  It calls the helper function evaluate_xgb_model() to test each model,
        and assigns the optimal hyperparameters to self.n_estimators and self.max_depth.
        '''
        n_estimators_list = [5,10,20,40,60,100,200,300,500]
        sqrt_n_features = int(round(np.sqrt(self.X_train.shape[1])))
        max_depths_list = range(sqrt_n_features - 5, sqrt_n_features + 5)
        
        warnings.filterwarnings("ignore")
        best_rmse, best_params = np.inf, None
        
        for max_depth in max_depths_list:
            for n_estimators in n_estimators_list:
                if verbose:
                    print 'Evaluating max_depth=%s, n_estimators=%s' % (max_depth, n_estimators)
                rmse = self.evaluate_xgb_model(max_depth, n_estimators)
                if rmse < best_rmse:
                    best_rmse, best_params = rmse, [max_depth, n_estimators]
                if verbose:
                    print('XGB%s RMSE=%s' % (best_params, rmse))
        if verbose:
            print 'Best XGB: [max_depth, n_estimators]=%s, RMSE=%s' % (best_params, best_rmse)
            
        self.max_depth = best_params[0]
        self.n_estimators = best_params[1]
    
    # Forecasting functions:

    def walk_forward_forecast(self, n_estimators=None, max_depth=None): 
        '''
        A function to perform a walk-forward forecast of the test data set.
        At each step the regressor produces a forecast of the target column
        one time sample in the future; records the error = (predicted value 
        minus observed value); and adds the observed value to the training set
        for use in the next forecasting step.
        '''
        if n_estimators == None:
            n_estimators = self.n_estimators
        if max_depth == None:
            max_depth = self.max_depth
        predictions = []
        for i in range(len(self.y_test)):
            model = XGBRegressor(n_estimators=self.n_estimators, max_depth=max_depth)
            model.fit(self.X[:self.split_idx + i, :], self.y[:self.split_idx + i])
            if i == 0:
                print model
            output = model.predict(self.X_test[i, :].reshape((1,-1)))
            prediction = output[0]
            predictions.append(prediction)
        error = mean_squared_error(self.y_test, predictions)
        rmse = np.sqrt(error)
        print 'Test RMSE: %.3f' % rmse
        self.rmse = rmse
        self.model = model
        self.predictions = predictions

    def get_feature_importances(self, model=None):
        '''
        A function to obtain the feature importances identified by the regression model.
        Output:
            - importances = the fractional importance of each predictor
            - names = the names of the columns corresponding to each predictor in the
              self.dataframe data set
            - indices = the indices of the importances, sorted from largest to smallest 
              fractional importance.
        '''
        if model == None:
            model = self.model
        importances = model.feature_importances_
        indices = np.argsort(importances)[::-1]
        names = np.array(self.train_columns)[indices]
        return names, importances, indices
    
    def generate_bootstrap_confidence_intervals(self, alpha=0.95, n_iterations=10): 
        '''
        A function to generate bootstrap confidence intervals for the regressor predictions
        by iteratively excluding data points, calculating a range of predictions, and 
        reporting the cutoff values for the 5th and 95th percentile predictions. 
          - The size of the confidence interval can be adjusted by changing alpha (e.g.,  
            set alpha=0.9 to obtain the bounds for a 90% confidence interval).
        Output: 
          - lowers = array of N lower bound values for the N model predictions
          - uppers = array of N upper bound values for the N model predictions
        '''
        uppers, lowers = [], []
        for i in range(len(self.y_test)):
            bootstrap_predictions = []
            for iteration in range(n_iterations):
                X_train = self.X[:self.split_idx+i,:]
                y_train = self.y[:self.split_idx+i]
                X_test = self.X[self.split_idx+i, :]
                indices = range(len(y_train))
                n_size = int(round(0.3*len(indices)))
                indices_to_exclude = np.unique(np.random.choice(indices, size=n_size))
                train_idxs = np.array([idx for idx in indices if idx not in indices_to_exclude])
                model = XGBRegressor(max_depth=self.max_depth, n_estimators=self.n_estimators)
                model.fit(X_train[train_idxs,:], y_train[train_idxs])
                output = model.predict(X_test.reshape((1,-1)))
                prediction = output[0]
                bootstrap_predictions.append(prediction)

            p = ((1. - alpha)/2.) * 100.
            lower = np.percentile(bootstrap_predictions, p)
            lowers.append(lower)

            p = (alpha + ((1. - alpha)/2.)) * 100.
            upper = np.percentile(bootstrap_predictions, p)
            uppers.append(upper)
        return np.array(lowers), np.array(uppers)
    
    # Plotting functions:

    def plot_forecast(self, lower_conf_intervals=[], upper_conf_intervals=[],
                      show=True, save=False):
        '''
        A function to plot the forecast generated by the walk_forward_forecast() function.
        To plot a shaded confidence interval, pass the output of the 
        generate_bootstrap_confidence_intervals() function as lower_conf_intervals 
        and upper_conf_intervals.
        '''
        xs = self.dataframe.mean_transaction_yearmonth.values
        xs = pd.to_datetime(xs)
        predictions = self.predictions
        ylabel = 'Monthly Avg Subsidy (USD)'
        if self.target_column == 'mean_transaction_amount':
            ys = self.dataframe.mean_transaction_amount
            rmse = self.rmse
        elif self.target_column == 'log_mean_transaction_amount':
            ys = df.log_mean_transaction_amount
            ylabel = 'log Monthly Avg Subsidy (log USD)'
            mse = mean_squared_error(y_test, np.exp(predictions))
            rmse = np.sqrt(mse)
        pl.plot(xs[:self.split_idx], ys[:self.split_idx], 'bo-', alpha=0.5, label='train')
        pl.plot(xs[self.split_idx:], ys[self.split_idx:], 'co-', alpha=1.0, label='test')
        pl.plot(xs[self.split_idx:], predictions, 'ro-', alpha=0.5, label='prediction')
        if len(lower_conf_intervals) > 0 and len(upper_conf_intervals) > 0:
            pl.fill_between(xs[self.split_idx:], lower_conf_intervals, upper_conf_intervals, 
                            color='r', alpha=0.2, label='95% prediction interval')
        pl.title('XGB Regressor Forecast: RMSE = %.2f' % rmse)
        pl.ylabel(ylabel)    
        pl.legend()
        pl.xticks(rotation=30, ha='right')
        if save:
            pl.savefig('XGB_regressor_forecast.png')
        if show:
            pl.show()
        
    def plot_feature_importances(self, names, importances, indices, show=True, save=False):
        '''
        A function to plot the feature importances output by the get_feature_importances() 
        function.
        '''
        pl.figure(figsize=(14,5))
        pl.title("Feature Importances (Top 10)")
        sns.set_context("poster", font_scale=1.5)
        sns.set_color_codes("muted")
        sns.barplot(importances[indices][:10], names[:10], palette='Blues_d')
        if save:
            pl.savefig('feature_importances.png')
        if show:
            pl.show()
    
    # Wrapper function:
    
    def run(self):
        '''
        A wrapper function to run the forecasting pipeline.
        '''
        self.form_training_and_test_sets()
        self.do_hyperparameter_grid_search()
        self.walk_forward_forecast()
        names, importances, indices = self.get_feature_importances()
        self.plot_feature_importances(names, importances, indices)
        lower_conf_intervals, upper_conf_intervals = self.generate_bootstrap_confidence_intervals()
        self.plot_forecast(lower_conf_intervals=lower_conf_intervals, 
                           upper_conf_intervals=upper_conf_intervals)